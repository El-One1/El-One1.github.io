---
layout: ../layouts/AboutLayout.astro
title: "About"
---

Hi! I'm a passionate graduate student in mathematics and statistics applied to deep-learning. This blog is about topics that I find interesting and little experiments to learn and grasp the beauty, magic and power of neural networks!

### Research Interests
My research interests lie in representation learning, explainability and knowledge behavior inside neural-networks. I enjoy building self-supervised architectures capable of learning rich latent-spaces from creative pretext tasks no matter the modality, and I thrive on trying to pinpoint where knowledge lies in complex networks to better control its flow during training and to better understand learning dynamics.

### Past studies and future goals
I'm currently studying at ETH Zürich in the Computer Science master's degree after a few years in France at Télécom Paris. In both universities I have studied maths, statistics and probabilities as well as computer-science and deep learning.

I intend to pursue a PhD from 2025 onwards in either of my fields of interest. I ultimately want to work on building more complex, capable systems and believe that new fundamental bricks are needed to reach more advanced intelligent systems. These bricks are what I dream to contribute to.

### Me in papers
Here are a few research papers that, as of May 2024, illustrate what I like reading:
#### The self-supervised representation learning series! (my personal favorite)
- [SimCLR from Chen et al.](https://arxiv.org/abs/2002.05709), the seminal contrastive-learning paper for images,
- [Byol from Grill et al.](https://arxiv.org/abs/2006.07733), an improvement especially in medical modalities with high inter-class similarity,
- [DINOv2 from Oquab et al.](https://arxiv.org/abs/2304.07193), a new strategy for learning and the appearance of Vision Transformers (ViT) (I should put DINOv1 in this list, too!),
- [V-JEPA from Bardes et al.](https://arxiv.org/abs/2404.08471), an architecture to learn rich representations from videos (see also JEPA as an introduction for V-JEPA, with images),
- [Contrastive Predictive Coding from van den Oord et al.](https://arxiv.org/abs/1807.03748), a cool architecture for time-series self-supervised learning.
#### The explainability series (TODO)

#### The knowledge understanding series (TODO)

<!--
<div>
  <img src="/assets/dev.svg" class="sm:w-1/2 mx-auto" alt="coding dev illustration">
</div>
-->